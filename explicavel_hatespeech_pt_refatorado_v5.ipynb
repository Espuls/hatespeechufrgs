{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "945e2c1d3f7102d5",
   "metadata": {},
   "source": [
    "# Uso do Aprendizado de Máquina Explicável na Detecção de Discursos de Ódio em Língua Portuguesa\n",
    "\n",
    "Este notebook implementa um pipeline completo para avaliação e explicabilidade de detectores de discurso de ódio (DO) em **português**, utilizando o corpus **OFFCOMBR** (variantes **OFFCOMBR-2** e **OFFCOMBR-3**). O script realiza carregamento, pré-processamento, treinamento e avaliação de três modelos, produz métricas e gráficos comparativos e gera análises de explicabilidade com SHAP:\n",
    "\n",
    "1. **Modelo clássico**: Random Forest (TF-IDF + n-grams + features linguísticas).\n",
    "2. **Modelo transformer**: **BERTimbau** (fine-tuning).\n",
    "3. **Modelo LLM**: LLaMA3 + LoRA (fine-tuning).\n",
    "\n",
    "**Estrutura do pipeline**\n",
    "1. Carregamento dos datasets OFFCOMBR-2 e OFFCOMBR-3 a partir de arquivos ARFF no repositório oficial.\n",
    "2. Pré-processamento em Português com normalização, remoção de ruído, substituição de gírias, lematização e correção ortográfica opcional.\n",
    "3. Geração de splits estratificados treino/teste.\n",
    "4. Treino e avaliação de modelos:\n",
    "    * Random Forest com pipeline TFIDF ngram + features linguísticas.\n",
    "    * BERTimbau fine-tuned via Hugging Face Trainer.\n",
    "    * LLaMA3 fine-tuned com LoRA usando PEFT e treino em 8bit quando disponível.\n",
    "5. Avaliação e visualização por dataset:\n",
    "    * Tabela de métricas: Accuracy, Precision, Recall, F1-score.\n",
    "    * Barplots comparativos.\n",
    "    * Curvas ROC e Precision Recall.\n",
    "    * Matrizes de confusão por modelo.\n",
    "6. Comparação entre OFFCOMBR-2 e OFFCOMBR-3:\n",
    "    * Tabela consolidada multiindex Dataset × Modelo.\n",
    "    * Estatísticas de distribuição de classes e proporção de desbalanceamento.\n",
    "    * Gráficos comparativos de métricas e heatmap de correlação.\n",
    "7. Explicabilidade com SHAP:\n",
    "    * TreeExplainer para Random Forest com feature importances (TFIDF + linguísticas).\n",
    "    * Explainer token-level para BERTimbau e LLaMA3+LoRA; resumo token importances e plots.\n",
    "    * Saída em CSV e PNG na pasta outputs/shap.\n",
    "\n",
    "**Arquivos de saída gerados**\n",
    "* ./results_* diretórios do Trainer para cada dataset e modelo.\n",
    "* ./outputs/shap/shap_rf_feature_importance.csv PNG e CSV para RF.\n",
    "* ./outputs/shap/shap_rf_summary.png.\n",
    "* ./outputs/shap/shap_bert_token_importance.csv.\n",
    "* ./outputs/shap/shap_bert_summary_bar.png.\n",
    "* ./outputs/shap/shap_llama_token_importance.csv (se LLaMA disponível).\n",
    "* ./outputs/shap/shap_llama_summary_bar.png (se LLaMA disponível).\n",
    "* Plots PNG para curvas ROC, PR, barplots, matrizes de confusão e heatmaps na sessão de execução.\n",
    "* Console prints contendo tabelas finais e estatísticas de distribuição.\n",
    "\n",
    "**Dependências e instalação**\n",
    "* Essenciais:\n",
    "    * Python 3.10+\n",
    "    * pip install -U pip setuptools\n",
    "* Pacotes Python:\n",
    "    * numpy pandas scikit-learn matplotlib seaborn liac-arff\n",
    "    * spacy pt_core_news_sm\n",
    "    * pyspellchecker\n",
    "    * datasets transformers\n",
    "    * torch\n",
    "* Para LLaMA3+LoRA (GPU recomendado):\n",
    "    * peft bitsandbytes accelerate\n",
    "    * transformers com suporte a bitsandbytes\n",
    "    * uma GPU com memória suficiente; recomenda-se CUDA compatível\n",
    "* Para explicabilidade:\n",
    "    * shap\n",
    "    * tqdm\n",
    "* Exemplo de instalação:\n",
    "    * pip install numpy pandas scikit-learn matplotlib seaborn liac-arff spacy pyspellchecker datasets transformers torch tqdm shap\n",
    "    * pip install peft bitsandbytes accelerate # apenas se for usar LoRA\n",
    "    * python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Carregamento do Dataset, Pré-processamento e Normalização refinados para português\n",
    "* Normalização de texto:\n",
    "    * Conversão para minúsculas.\n",
    "    * Remoção de acentos (unidecode).\n",
    "    * Substituição de caracteres especiais.\n",
    "    * Expansão de contrações (ex.: “vc” → “você”).\n",
    "* Limpeza:\n",
    "    * Remoção de URLs, menções (@usuario), hashtags e emojis.\n",
    "    * Remoção de Stopwords.\n",
    "* Tokenização:\n",
    "    * Segmentação em palavras ou subpalavras (BPE, SentencePiece), especialmente útil em português por causa da morfologia rica.\n",
    "    * Usa spaCy (pt_core_news_sm) para segmentação mais robusta.\n",
    "* Lematização:\n",
    "    * Melhor que stemming para preservar sentido.\n",
    "    * Redução de palavras à forma base (ex.: “xingando” → “xingar”).\n",
    "* Correção ortográfica e gírias:\n",
    "    * Biblioteca pyspellchecker para português.\n",
    "    * Dicionário customizado de gírias → forma padrão (ex.: “vc” → “você”, “pq” → “porque”)."
   ],
   "id": "80b53f436215b122"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Random Forest + Feature Engineering Tradicional\n",
    "* Bag-of-Words (BoW): baseline simples.\n",
    "* TF-IDF: mais robusto para capturar relevância.\n",
    "* N-grams: bigramas e trigramas são úteis em discurso de ódio (ex.: “vai morrer”, “seu lixo”):\n",
    "    * Sugestão: usar 1-3 n-grams.\n",
    "* Features linguísticas adicionais:\n",
    "    * Contagem de palavrões (lista customizada).\n",
    "    * Número de maiúsculas.\n",
    "    * Número de pontos de exclamação.\n",
    "    * Emojis detectados."
   ],
   "id": "2e466c14f0cd7f34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BERTimbau (Transformer - variação do BERT)\n",
    "* Usa transformers (Hugging Face).\n",
    "* Estratégia: fine-tuning em BERTimbau-base ou BERTimbau-large.\n",
    "* Entrada: texto cru (sem lematização/stemming, apenas limpeza mínima).\n",
    "* Saída: classificação binária.\n",
    "* Word embeddings pré-treinados:\n",
    "    * Word2Vec, FastText (bom para português por lidar com morfologia e palavras raras).\n",
    "* Contextual embeddings:\n",
    "    * BERTimbau e GPT-like models ajustados para português.\n",
    "* Fine-tuning de Transformers:\n",
    "    * Ajuste fino em datasets anotados de discurso de ódio.\n",
    "* Data augmentation:\n",
    "    * Parafraseamento, tradução ida-e-volta (back-translation), substituição por sinônimos para lidar com desbalanceamento de classes."
   ],
   "id": "ed74274da8e366a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Observações\n",
    "* Random Forest: baseline clássico, útil para entender ganhos relativos.\n",
    "* BERTimbau: modelo contextual já pré-treinado em português, se adapta bem ao domínio devido ao ajuste via fine-tuning.\n",
    "* LLaMA3 + LoRA: não utilizado apenas como um modelo genérico em prompting, foi adaptado ao OFFCOMBR, aprendendo padrões ofensivos específicos do português.\n",
    "    * Com isso, o LLaMA3 ganha competitividade real contra o BERTimbau.\n",
    "    * A comparação passa a ser mais justa, porque ambos (BERTimbau e LLaMA3) estão sendo fine-tunados no mesmo dataset.\n",
    "    * A diferença é que, com LoRA, o LLaMA3 deixa de ser apenas um baseline de prompting e passa a ser um concorrente direto do BERTimbau.\n",
    "    * Isso enriquece a análise, porque podemos ver quanto o fine-tuning eficiente (LoRA) melhora um LLM genérico em relação a um modelo já especializado em português (BERTimbau)."
   ],
   "id": "a796a52a568b1a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Avaliação dos modelos\n",
    "* Comparação de métricas.\n",
    "* Curvas ROC e AUC.\n",
    "* Curvas Precision-Recall (PRC).\n",
    "* Barplot comparativo."
   ],
   "id": "8cb984e5373eebf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Versão CPU + GPU",
   "id": "68f1680799944955"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T22:19:58.301043Z",
     "start_time": "2025-10-24T22:19:56.804803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Pipeline end-to-end único:\n",
    "1) Carrega OFFCOMBR-2 e OFFCOMBR-3\n",
    "2) Pré-processa (lemmatização, normalização, correção opcional)\n",
    "3) Gera splits estratificados\n",
    "4) Treina/avalia: Random Forest, BERTimbau, LLaMA3+LoRA\n",
    "5) Plota métricas, ROC, PRC, matrizes de confusão por dataset\n",
    "6) Consolida comparação entre OFFCOMBR-2 e OFFCOMBR-3 (tabela, distribuição, gráficos, heatmap)\n",
    "7) Explicabilidade com SHAP (RF, BERTimbau, LLaMA quando disponível)\n",
    "Notas:\n",
    "- Ajuste hiperparâmetros, caminhos e batch sizes conforme sua infra.\n",
    "- Requer pacotes listados no README associado.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# 0. Imports gerais e detecção de GPU\n",
    "# =========================\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import unidecode\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arff\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Hugging Face / PEFT / SHAP\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import shap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.max_open_warning\": 0})\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(f\"USE_CUDA={USE_CUDA}, device={device}\")\n",
    "\n",
    "# =========================\n",
    "# 1. Carregamento OFFCOMBR\n",
    "# =========================\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/rogersdepelle/OffComBR/master\"\n",
    "ARFF_URLS = {\n",
    "    \"offcombr-2\": f\"{GITHUB_RAW_BASE}/offcombr2.arff\",\n",
    "    \"offcombr-3\": f\"{GITHUB_RAW_BASE}/offcombr3.arff\",\n",
    "}\n",
    "\n",
    "def load_offcombr(cfg=\"offcombr-2\"):\n",
    "    if cfg not in ARFF_URLS:\n",
    "        raise ValueError(\"Use 'offcombr-2' ou 'offcombr-3'\")\n",
    "    raw = urllib.request.urlopen(ARFF_URLS[cfg]).read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    obj = arff.loads(raw)\n",
    "    cols = [a[0] for a in obj[\"attributes\"]]\n",
    "    df = pd.DataFrame(obj[\"data\"], columns=cols)\n",
    "    if \"document\" in df.columns:\n",
    "        df = df.rename(columns={\"document\": \"text\"})\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    df = df[df[\"text\"] != \"\"].copy()\n",
    "    class_col = next((c for c in df.columns if \"class\" in c.lower()), None)\n",
    "    if class_col is None:\n",
    "        raise ValueError(\"Não foi encontrada coluna de classe no ARFF\")\n",
    "    df[\"label_int\"] = df[class_col].astype(str).str.lower().map({\"no\":0,\"yes\":1,\"not\":0,\"offensive\":1}).astype(int)\n",
    "    return df[[\"text\",\"label_int\"]]\n",
    "\n",
    "# =========================\n",
    "# 2. Pré-processamento (Português)\n",
    "# =========================\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "spell = SpellChecker(language=\"pt\")\n",
    "giria_dict = {\"vc\":\"você\",\"pq\":\"porque\",\"tbm\":\"também\",\"q\":\"que\"}\n",
    "\n",
    "def preprocess_text_pt(text, do_spell=True):\n",
    "    text = str(text).lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"http\\S+|@\\w+|#\\w+|\\s+\", \" \", text)\n",
    "    for g, norm in giria_dict.items():\n",
    "        text = re.sub(rf\"\\b{g}\\b\", norm, text)\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemma = token.lemma_\n",
    "            if do_spell:\n",
    "                corr = spell.correction(token.text)\n",
    "                tokens.append(corr if corr is not None and corr != token.text else lemma)\n",
    "            else:\n",
    "                tokens.append(lemma)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def make_splits(df, test_size=0.2, seed=42):\n",
    "    X = df[\"text\"].apply(preprocess_text_pt)\n",
    "    y = df[\"label_int\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X.values, y.values, test_size=test_size, stratify=y.values, random_state=seed\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# =========================\n",
    "# 3. Modelos: Random Forest pipeline\n",
    "# =========================\n",
    "class LinguisticFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        feats = []\n",
    "        for text in X:\n",
    "            toks = text.split()\n",
    "            insults = sum(1 for w in toks if w in {\"idiota\",\"burro\",\"lixo\",\"estúpido\",\"imbecil\"})\n",
    "            upper_chars = sum(1 for c in text if c.isupper())\n",
    "            excl = text.count(\"!\")\n",
    "            feats.append([insults, upper_chars, excl])\n",
    "        return np.array(feats)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=10000)\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"features\", FeatureUnion([(\"tfidf\", tfidf), (\"ling\", LinguisticFeatures())])),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 4. Função genérica de avaliação\n",
    "# =========================\n",
    "def avaliar_modelo(nome, modelo, X_test_raw, y_test, tipo=\"sklearn\"):\n",
    "    if tipo == \"sklearn\":\n",
    "        y_pred = modelo.predict(X_test_raw)\n",
    "        y_score = modelo.predict_proba(X_test_raw)[:,1]\n",
    "    elif tipo == \"trainer\":\n",
    "        preds = modelo.predict(X_test_raw)  # X_test_raw espera HF Dataset (tokenizado)\n",
    "        logits = preds.predictions\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[:,0,:]\n",
    "        y_score = logits[:,1]\n",
    "        y_pred = logits.argmax(axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Tipo inválido\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"Modelo\": nome, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-score\": f1, \"y_score\": y_score}\n",
    "\n",
    "# =========================\n",
    "# 5. Função LLaMA3 + LoRA (GPU-aware)\n",
    "# =========================\n",
    "def treinar_llama_lora(cfg, X_train, y_train, X_eval, y_eval, output_dir_base=\"./results_llama\"):\n",
    "    model_name = \"meta-llama/Llama-3-8b-instruct\"  # substitua conforme o checkpoint do modelo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    max_len = 256\n",
    "\n",
    "    ds_train = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train})\n",
    "    ds_eval  = Dataset.from_dict({\"text\": X_eval.tolist(), \"label\": y_eval})\n",
    "    def tokenize_fn(batch): return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "    ds_eval  = ds_eval.map(tokenize_fn, batched=True)\n",
    "    ds_train.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    ds_eval.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "    # Carrega o modelo em 8-bit e usa device_map para colocar na GPU\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Prepara para treinamento k-bit\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    outdir = os.path.join(output_dir_base, cfg, \"llama_lora\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Se CUDA estiver disponível, habilita fp16; caso contrário roda em fp32\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=outdir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=USE_CUDA,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_eval,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(ds_eval)\n",
    "    logits = preds.predictions\n",
    "    if logits.ndim == 3:\n",
    "        logits = logits[:,0,:]\n",
    "    y_score = logits[:,1]\n",
    "    y_pred = logits.argmax(axis=-1)\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_eval, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"Modelo\": \"LLaMA3 + LoRA\", \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-score\": f1, \"y_score\": y_score}\n",
    "\n",
    "# =========================\n",
    "# 6. Loop principal por dataset: treino e avaliação\n",
    "# =========================\n",
    "comparacao_datasets = {}\n",
    "comparacao_datasets_raw = {}\n",
    "\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    print(f\"\\n--- Pipeline para {cfg} ---\")\n",
    "    df = load_offcombr(cfg)\n",
    "    comparacao_datasets_raw[cfg] = df.copy()\n",
    "    X_train, X_test, y_train, y_test = make_splits(df)\n",
    "\n",
    "    # Random Forest (CPU)\n",
    "    print(\"Treinando Random Forest...\")\n",
    "    pipeline_rf.fit(X_train, y_train)\n",
    "    res_rf = avaliar_modelo(\"Random Forest (TF-IDF+n-grams)\", pipeline_rf, X_test, y_test, tipo=\"sklearn\")\n",
    "\n",
    "    # BERTimbau (GPU)\n",
    "    print(\"Treinando BERTimbau (uso de GPU quando disponível)...\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "    ds_train = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train})\n",
    "    ds_test  = Dataset.from_dict({\"text\": X_test.tolist(), \"label\": y_test})\n",
    "    def tokenize_fn(batch): return tokenizer_bert(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "    ds_test  = ds_test.map(tokenize_fn, batched=True)\n",
    "    ds_train.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    ds_test.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "    model_bert = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)\n",
    "    # Move o modelo para device (GPU se disponível)\n",
    "    model_bert.to(device)\n",
    "\n",
    "    # Habilita fp16 se CUDA estiver disponível\n",
    "    bert_training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{cfg}/bert\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=USE_CUDA,\n",
    "    )\n",
    "    trainer_bert = Trainer(model=model_bert, args=bert_training_args, train_dataset=ds_train, eval_dataset=ds_test, tokenizer=tokenizer_bert)\n",
    "    trainer_bert.train()\n",
    "    res_bert = avaliar_modelo(\"BERTimbau (fine-tune)\", trainer_bert, ds_test, y_test, tipo=\"trainer\")\n",
    "\n",
    "    # LLaMA3 + LoRA (GPU)\n",
    "    print(\"Treinando LLaMA3 + LoRA (GPU se disponível)...\")\n",
    "    try:\n",
    "        res_llama = treinar_llama_lora(cfg, X_train, y_train, X_test, y_test)\n",
    "        resultados = [res_rf, res_bert, res_llama]\n",
    "    except Exception as e:\n",
    "        print(\"LLaMA3+LoRA falhou ou indisponível:\", e)\n",
    "        resultados = [res_rf, res_bert]\n",
    "\n",
    "    comparacao_datasets[cfg] = pd.DataFrame([{k:v for k,v in r.items() if k!=\"y_score\"} for r in resultados])\n",
    "    comparacao_datasets[cfg + \"_raw_results\"] = resultados\n",
    "    comparacao_datasets[cfg + \"_ytest\"] = y_test\n",
    "    comparacao_datasets[cfg + \"_Xtest\"] = X_test\n",
    "\n",
    "# =========================\n",
    "# 7. Seção de avaliação por dataset (métricas, ROC, PR, conf matrix)\n",
    "# =========================\n",
    "def avaliar_resultados(resultados, y_test, cfg_name):\n",
    "    df_resultados = pd.DataFrame([{k:v for k,v in r.items() if k!=\"y_score\"} for r in resultados])\n",
    "    print(f\"\\n=== Comparação de Modelos - {cfg_name.upper()} ===\")\n",
    "    print(df_resultados)\n",
    "\n",
    "    # Barplot\n",
    "    melted = df_resultados.melt(id_vars=\"Modelo\", var_name=\"Métrica\", value_name=\"Valor\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=melted[melted[\"Métrica\"]!=\"Modelo\"], x=\"Métrica\", y=\"Valor\", hue=\"Modelo\")\n",
    "    plt.title(f\"Comparação de Modelos - {cfg_name.upper()}\")\n",
    "    plt.ylim(0,1)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for r in resultados:\n",
    "        fpr, tpr, _ = roc_curve(y_test, r[\"y_score\"])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"{r['Modelo']} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0,1],[0,1], color=\"gray\", linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Curvas ROC - {cfg_name.upper()}\"); plt.legend(loc=\"lower right\"); plt.show()\n",
    "\n",
    "    # PRC\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for r in resultados:\n",
    "        precision, recall, _ = precision_recall_curve(y_test, r[\"y_score\"])\n",
    "        ap = average_precision_score(y_test, r[\"y_score\"])\n",
    "        plt.plot(recall, precision, lw=2, label=f\"{r['Modelo']} (AP = {ap:.2f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Curvas Precision-Recall - {cfg_name.upper()}\"); plt.legend(loc=\"lower left\"); plt.show()\n",
    "\n",
    "    # Matriz de Confusão\n",
    "    for r in resultados:\n",
    "        y_pred = (r[\"y_score\"] >= 0.5).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=[\"Não-ofensivo\",\"Ofensivo\"],\n",
    "                    yticklabels=[\"Não-ofensivo\",\"Ofensivo\"])\n",
    "        plt.title(f\"Matriz de Confusão - {r['Modelo']} ({cfg_name.upper()})\")\n",
    "        plt.ylabel(\"Verdadeiro\"); plt.xlabel(\"Predito\"); plt.show()\n",
    "\n",
    "    return df_resultados\n",
    "\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    resultados = comparacao_datasets[cfg + \"_raw_results\"]\n",
    "    y_test = comparacao_datasets[cfg + \"_ytest\"]\n",
    "    _ = avaliar_resultados(resultados, y_test, cfg)\n",
    "\n",
    "# =========================\n",
    "# 8. Comparação entre datasets (tabela consolidada, distribuição, gráficos, heatmap)\n",
    "# =========================\n",
    "tabela_final = pd.concat(\n",
    "    {k: v.set_index(\"Modelo\") for k, v in comparacao_datasets.items() if not k.endswith(\"_raw_results\") and not k.endswith(\"_ytest\") and not k.endswith(\"_Xtest\")},\n",
    "    axis=0\n",
    ")\n",
    "print(\"\\n=== Tabela consolidada (OFFCOMBR-2 vs OFFCOMBR-3) ===\")\n",
    "print(tabela_final)\n",
    "\n",
    "# Estatísticas de distribuição\n",
    "print(\"\\n=== Distribuição de classes e desbalanceamento ===\")\n",
    "dist_stats = []\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    df_raw = comparacao_datasets_raw[cfg]\n",
    "    total = len(df_raw)\n",
    "    ofensivos = int(df_raw[\"label_int\"].sum())\n",
    "    nao_ofensivos = total - ofensivos\n",
    "    pct_of = ofensivos / total\n",
    "    ratio = ofensivos / (nao_ofensivos if nao_ofensivos > 0 else 1)\n",
    "    dist_stats.append({\"Dataset\": cfg, \"Total\": total, \"Ofensivo\": ofensivos, \"Não-ofensivo\": nao_ofensivos,\n",
    "                       \"Pct_ofensivo\": pct_of, \"Pct_nao_ofensivo\": 1-pct_of, \"Ratio_of/nao\": ratio})\n",
    "    print(f\"{cfg.upper()}: total={total} | ofensivo={ofensivos} ({pct_of:.1%}) | não-ofensivo={nao_ofensivos} ({1-pct_of:.1%}) | ratio={ratio:.2f}\")\n",
    "\n",
    "df_dist = pd.DataFrame(dist_stats).set_index(\"Dataset\")\n",
    "\n",
    "# Gráficos comparativos\n",
    "plot_df = tabela_final.reset_index().rename(columns={\"level_0\":\"Dataset\"}).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=plot_df, x=\"Modelo\", y=\"F1-score\", hue=\"Dataset\")\n",
    "plt.title(\"Comparação de F1-score entre OFFCOMBR-2 e OFFCOMBR-3\"); plt.ylim(0,1); plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=plot_df, x=\"Modelo\", y=\"Accuracy\", hue=\"Dataset\")\n",
    "plt.title(\"Comparação de Accuracy entre OFFCOMBR-2 e OFFCOMBR-3\"); plt.ylim(0,1); plt.show()\n",
    "\n",
    "# Heatmap de correlação entre métricas\n",
    "num_cols = tabela_final.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(tabela_final[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlação entre métricas (OFFCOMBR-2 + OFFCOMBR-3)\"); plt.show()\n",
    "\n",
    "# Distribuição de classes plot\n",
    "plt.figure(figsize=(8,5))\n",
    "df_dist_plot = df_dist.reset_index()\n",
    "sns.barplot(data=df_dist_plot.melt(id_vars=\"Dataset\", value_vars=[\"Pct_ofensivo\",\"Pct_nao_ofensivo\"]),\n",
    "            x=\"Dataset\", y=\"value\", hue=\"variable\")\n",
    "plt.ylim(0,1); plt.ylabel(\"Proporção\"); plt.title(\"Proporção de classes por dataset\"); plt.show()\n",
    "\n",
    "# Mostrar tabelas finais\n",
    "print(\"\\n--- Tabela final (para leitura) ---\")\n",
    "print(tabela_final.reset_index().rename(columns={\"level_0\":\"Dataset\"}))\n",
    "print(\"\\n--- Distribuição de classes ---\")\n",
    "print(df_dist)\n",
    "\n",
    "# =========================\n",
    "# 9. Explicabilidade com SHAP (gera CSVs e PNGs em ./outputs/shap/)\n",
    "# =========================\n",
    "os.makedirs(\"./outputs/shap\", exist_ok=True)\n",
    "\n",
    "# Parâmetros ajustáveis (reduzir para acelerar)\n",
    "n_background = 50    # tamanho da amostra de fundo para explainers de kernel\n",
    "n_explain = 100      # quantos exemplos do test set explicar (subamostrar)\n",
    "n_samples_kernel = 1024  # nsamples para KernelExplainer (quanto maior, mais estável/mais lento)\n",
    "\n",
    "# Helper: obter textos de teste (subsample)\n",
    "def sample_test_texts(X_test, n):\n",
    "    n = min(n, len(X_test))\n",
    "    idx = np.random.default_rng(42).choice(len(X_test), size=n, replace=False)\n",
    "    return X_test[idx], idx\n",
    "\n",
    "# -------------------------\n",
    "# 9.1. Random Forest (TreeExplainer)\n",
    "# -------------------------\n",
    "try:\n",
    "    print(\"\\n=== SHAP: Random Forest ===\")\n",
    "    # Para TreeExplainer precisamos do transform interno (TF-IDF) + features numéricas.\n",
    "    # Criado uma wrapper que recebe raw text array e retorna predict_proba.\n",
    "    def rf_proba(texts):\n",
    "        return pipeline_rf.predict_proba(texts)\n",
    "\n",
    "    # background: amostra de treino (pré-processado) — usar X_train já disponível no loop; escolhido do último dataset ou concatenado\n",
    "    # Escolhido X_train do offcombr-2 se disponível, senão do primeiro dataset\n",
    "    any_cfg = \"offcombr-2\" if \"offcombr-2\" in comparacao_datasets else list(comparacao_datasets.keys())[0]\n",
    "    X_train_ref = comparacao_datasets[any_cfg + \"_Xtest\"] if any_cfg + \"_Xtest\" in comparacao_datasets else X_train\n",
    "    background_texts = np.random.choice(X_train_ref, size=min(n_background, len(X_train_ref)), replace=False)\n",
    "\n",
    "    # Aplica masker/text se quiser explicações no nível de tokens não faz sentido para TF-IDF features;\n",
    "    # usado explainer de árvore direto sobre pipeline (shap.TreeExplainer aceita sklearn pipelines)\n",
    "    explainer_rf = shap.TreeExplainer(pipeline_rf.named_steps[\"clf\"])  # TreeExplainer sobre o estimador\n",
    "    # Para obter valores SHAP para as features TF-IDF + linguísticas precisamos extrair matriz de features:\n",
    "    X_test_full_feats = pipeline_rf.named_steps[\"features\"].transform(comparacao_datasets[any_cfg + \"_Xtest\"])\n",
    "    # shap expects 2d array\n",
    "    shap_values_rf = explainer_rf.shap_values(X_test_full_feats, check_additivity=False)\n",
    "\n",
    "    # shap_values_rf é uma lista (um por classe) — usamos classe 1 (ofensivo)\n",
    "    sv_rf_pos = shap_values_rf[1] if isinstance(shap_values_rf, list) else shap_values_rf\n",
    "    # Agrega importâncias (valor absoluto médio por feature)\n",
    "    mean_abs = np.abs(sv_rf_pos).mean(axis=0)\n",
    "    # Reconstrói nomes de features: TF-IDF feature names + linguistic feature names\n",
    "    tfidf_names = pipeline_rf.named_steps[\"features\"].transformer_list[0][1].get_feature_names_out()\n",
    "    ling_names = [\"insults_count\",\"upper_chars\",\"exclamations\"]\n",
    "    feat_names = list(tfidf_names) + ling_names\n",
    "    df_shap_rf = pd.DataFrame({\"feature\": feat_names, \"mean_abs_shap\": mean_abs})\n",
    "    df_shap_rf = df_shap_rf.sort_values(\"mean_abs_shap\", ascending=False).reset_index(drop=True)\n",
    "    df_shap_rf.to_csv(\"./outputs/shap/shap_rf_feature_importance.csv\", index=False)\n",
    "    print(\"Saved ./outputs/shap/shap_rf_feature_importance.csv\")\n",
    "\n",
    "    # Summary plot (top 30 features)\n",
    "    top_k = min(30, len(feat_names))\n",
    "    plt.figure(figsize=(8,10))\n",
    "    shap.summary_plot(sv_rf_pos, X_test_full_feats, feature_names=feat_names, max_display=top_k, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./outputs/shap/shap_rf_summary.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved ./outputs/shap/shap_rf_summary.png\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP Random Forest falhou:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# 9.2. BERTimbau (Transformer) — KernelExplainer com wrapper sobre Trainer.predict ou model pipeline\n",
    "# -------------------------\n",
    "try:\n",
    "    print(\"\\n=== SHAP: BERTimbau ===\")\n",
    "    # Localiza trainer/BERT treinado: usado trainer_bert do último loop (treinado por cfg = offcombr-3 por fim)\n",
    "    # Treinado para ambos datasets, pego objeto trainer_bert do último loop; definido wrapper predict_proba_texts\n",
    "    def bert_predict_proba(texts):\n",
    "        # texts: list/np.array de raw strings\n",
    "        enc = tokenizer_bert(list(texts), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        # Move tensores para device\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        model_bert.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_bert(**enc)\n",
    "            probs = torch.softmax(out.logits, dim=-1)[:,1].detach().cpu().numpy()\n",
    "        # KernelExplainer espera shape (n_samples, ) or (n_samples, n_outputs)\n",
    "        return np.vstack([1-probs, probs]).T\n",
    "\n",
    "    # Prepara background (pequena amostra de textos)\n",
    "    # Usa comparacao_datasets_raw do último dataset (offcombr-3 se existir)\n",
    "    bg_cfg = \"offcombr-3\" if \"offcombr-3\" in comparacao_datasets_raw else list(comparacao_datasets_raw.keys())[0]\n",
    "    bg_texts = np.random.choice(comparacao_datasets_raw[bg_cfg][\"text\"].values, size=min(n_background, len(comparacao_datasets_raw[bg_cfg])), replace=False)\n",
    "\n",
    "    # Seleciona textos para explicar (do test set)\n",
    "    test_texts, test_idx = sample_test_texts(comparacao_datasets[bg_cfg + \"_Xtest\"] if bg_cfg + \"_Xtest\" in comparacao_datasets else X_test, n_explain)\n",
    "\n",
    "    # KernelExplainer wrapper: usa função que retorna probas para classe 1; shap.KernelExplainer aceita função retornando uma escalar ou vetor\n",
    "    # Criar função que aceita a list de strings e retorna probas positivas\n",
    "    def bert_fn(texts):\n",
    "        return bert_predict_proba(texts)\n",
    "\n",
    "    # Usa text masker para acelerar e permitir token masking; KernelExplainer ignora masker mas shap.Explainer com masker.Text pode auto-selecionar explainer\n",
    "    # Usa shap.Explainer (auto) com masker text:\n",
    "    masker = shap.maskers.Text(tokenizer_bert)\n",
    "    explainer_bert = shap.Explainer(lambda x: bert_predict_proba(x), masker, output_names=[\"non-off\",\"offensive\"])\n",
    "    # Explica (pode ser demorado)\n",
    "    shap_values_bert = explainer_bert(test_texts)\n",
    "\n",
    "    # Agrega importâncias token-level: para cada sample, shap_values_bert.data (tokens) e shap_values_bert.values (len toks x outputs)\n",
    "    # Vamos gerar uma tabela que resume importância média absoluta por token text através de explained samples\n",
    "    tokens_list = []\n",
    "    for i in range(len(test_texts)):\n",
    "        toks = shap_values_bert.data[i]\n",
    "        vals = shap_values_bert.values[i][:,1]  # classe positiva\n",
    "        for t, v in zip(toks, vals):\n",
    "            tokens_list.append({\"text\": test_texts[i], \"token\": t, \"shap_abs\": abs(v), \"shap\": v})\n",
    "    df_tokens = pd.DataFrame(tokens_list)\n",
    "    df_token_summary = df_tokens.groupby(\"token\")[\"shap_abs\"].mean().sort_values(ascending=False).reset_index().rename(columns={\"shap_abs\":\"mean_abs_shap\"})\n",
    "    df_token_summary.to_csv(\"./outputs/shap/shap_bert_token_importance.csv\", index=False)\n",
    "    print(\"Saved ./outputs/shap/shap_bert_token_importance.csv\")\n",
    "\n",
    "    # Summary plot (per-sample token-level summary)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    shap.plots.bar(shap_values_bert[:, :, 1], max_display=30, show=False)  # class index 1\n",
    "    plt.title(\"BERTimbau token-level SHAP (class=offensive)\")\n",
    "    plt.savefig(\"./outputs/shap/shap_bert_summary_bar.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved ./outputs/shap/shap_bert_summary_bar.png\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP BERTimbau falhou:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# 9.3. LLaMA3 + LoRA (Transformer via KernelExplainer wrapper)\n",
    "# -------------------------\n",
    "try:\n",
    "    # Verficase existem resultados do LLaMA\n",
    "    has_llama = any(\"LLaMA3 + LoRA\" in r[\"Modelo\"] for k in comparacao_datasets.keys() for r in (comparacao_datasets.get(k + \"_raw_results\", []) if isinstance(comparacao_datasets.get(k + \"_raw_results\", []), list) else []))\n",
    "    if has_llama:\n",
    "        print(\"\\n=== SHAP: LLaMA3 + LoRA ===\")\n",
    "        # Procura qual cfg tem os resultados do LLaMA\n",
    "        llama_cfg = None\n",
    "        for cfg in [\"offcombr-2\",\"offcombr-3\"]:\n",
    "            raw = comparacao_datasets.get(cfg + \"_raw_results\", [])\n",
    "            if any(r[\"Modelo\"] == \"LLaMA3 + LoRA\" for r in raw):\n",
    "                llama_cfg = cfg\n",
    "                break\n",
    "        if llama_cfg is None:\n",
    "            raise RuntimeError(\"Não foi encontrado LLaMA3+LoRA nos resultados\")\n",
    "\n",
    "        # Build wrapper: usa trainer retornado de treinar_llama_lora ou usa trainer salvo, caso salvo.\n",
    "        # Rebuild da função de predição usando AutoTokenizer + get_peft_model modelo carregado, caso disponível.\n",
    "        # Approach simples: reusa as predições que já foram computadas (y_score e logits) — mas SHAP precisa de acesso ao modelo.\n",
    "        # Criado um wrapper que carrega o modelo LLaMA (mode de inferência) e retorna as probabilidades para a lista de textos.\n",
    "        model_name = \"meta-llama/Llama-3-8b-instruct\"  # mesmo do trainamento\n",
    "        tokenizer_llama = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        # Carrega o modelo em 8bit + device_map auto (para inferência)\n",
    "        llama_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, load_in_8bit=True, device_map=\"auto\")\n",
    "        # Se PEFT weights salvo, carregue peft weights: get_peft_model + model.load_state_dict ou use PeftModel.from_pretrained\n",
    "\n",
    "        def llama_predict_proba(texts):\n",
    "            enc = tokenizer_llama(list(texts), padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                out = llama_model(**enc)\n",
    "                probs = torch.softmax(out.logits, dim=-1)[:,1].detach().cpu().numpy()\n",
    "            return np.vstack([1-probs, probs]).T\n",
    "\n",
    "        # Background e textos de teste\n",
    "        bg_cfg = llama_cfg\n",
    "        bg_texts = np.random.choice(comparacao_datasets_raw[bg_cfg][\"text\"].values, size=min(n_background, len(comparacao_datasets_raw[bg_cfg])), replace=False)\n",
    "        test_texts, test_idx = sample_test_texts(comparacao_datasets[bg_cfg + \"_Xtest\"], n_explain)\n",
    "\n",
    "        masker = shap.maskers.Text(tokenizer_llama)\n",
    "        explainer_llama = shap.Explainer(lambda x: llama_predict_proba(x), masker, output_names=[\"non-off\",\"offensive\"])\n",
    "        shap_values_llama = explainer_llama(test_texts)\n",
    "\n",
    "        # Importância de nível de tokens agregada\n",
    "        tokens_list = []\n",
    "        for i in range(len(test_texts)):\n",
    "            toks = shap_values_llama.data[i]\n",
    "            vals = shap_values_llama.values[i][:,1]\n",
    "            for t, v in zip(toks, vals):\n",
    "                tokens_list.append({\"text\": test_texts[i], \"token\": t, \"shap_abs\": abs(v), \"shap\": v})\n",
    "        df_tokens_llama = pd.DataFrame(tokens_list)\n",
    "        df_token_summary_llama = df_tokens_llama.groupby(\"token\")[\"shap_abs\"].mean().sort_values(ascending=False).reset_index().rename(columns={\"shap_abs\":\"mean_abs_shap\"})\n",
    "        df_token_summary_llama.to_csv(\"./outputs/shap/shap_llama_token_importance.csv\", index=False)\n",
    "        print(\"Saved ./outputs/shap/shap_llama_token_importance.csv\")\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        shap.plots.bar(shap_values_llama[:, :, 1], max_display=30, show=False)\n",
    "        plt.title(\"LLaMA3+LoRA token-level SHAP (class=offensive)\")\n",
    "        plt.savefig(\"./outputs/shap/shap_llama_summary_bar.png\", dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved ./outputs/shap/shap_llama_summary_bar.png\")\n",
    "    else:\n",
    "        print(\"Nenhum resultado LLaMA3+LoRA detectado; pulando SHAP para LLaMA.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP LLaMA3+LoRA falhou:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# 9.4. Consolidação: tabelas resumidas por modelo\n",
    "# -------------------------\n",
    "try:\n",
    "    print(\"\\n=== Consolidação de tabelas SHAP ===\")\n",
    "    # Tabela RF já foi salva; Tabela de tokens BERT/LLaMA salva.\n",
    "    # Carrega e mostra os top-20 de cada\n",
    "    if os.path.exists(\"./outputs/shap/shap_rf_feature_importance.csv\"):\n",
    "        df_rf_shap = pd.read_csv(\"./outputs/shap/shap_rf_feature_importance.csv\").head(20)\n",
    "        print(\"\\nTop RF features (SHAP):\")\n",
    "        print(df_rf_shap)\n",
    "\n",
    "    if os.path.exists(\"./outputs/shap/shap_bert_token_importance.csv\"):\n",
    "        df_bert_tok = pd.read_csv(\"./outputs/shap/shap_bert_token_importance.csv\").head(30)\n",
    "        print(\"\\nTop BERT tokens (SHAP):\")\n",
    "        print(df_bert_tok)\n",
    "\n",
    "    if os.path.exists(\"./outputs/shap/shap_llama_token_importance.csv\"):\n",
    "        df_llama_tok = pd.read_csv(\"./outputs/shap/shap_llama_token_importance.csv\").head(30)\n",
    "        print(\"\\nTop LLaMA tokens (SHAP):\")\n",
    "        print(df_llama_tok)\n",
    "except Exception as e:\n",
    "    print(\"Consolidação SHAP falhou:\", e)\n",
    "\n",
    "print(\"\\nSHAP analysis complete. Outputs saved to ./outputs/shap/\")\n",
    "\n",
    "# =========================\n",
    "# Fim do script\n",
    "# ========================="
   ],
   "id": "c86760f800bf66a5",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'arff'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msns\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01marff\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mspacy\u001B[39;00m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mshap\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'arff'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T17:16:36.960210Z",
     "start_time": "2025-10-25T17:16:16.067768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Pipeline end-to-end único:\n",
    "1) Carrega OFFCOMBR-2 e OFFCOMBR-3\n",
    "2) Pré-processa (lemmatização, normalização, correção opcional)\n",
    "3) Gera splits estratificados\n",
    "4) Treina/avalia: Random Forest, BERTimbau, LLaMA3+LoRA\n",
    "5) Plota métricas, ROC, PRC, matrizes de confusão por dataset\n",
    "6) Consolida comparação entre OFFCOMBR-2 e OFFCOMBR-3 (tabela, distribuição, gráficos, heatmap)\n",
    "7) Explicabilidade com SHAP (RF, BERTimbau, LLaMA quando disponível)\n",
    "Notas:\n",
    "- Ajuste hiperparâmetros, caminhos e batch sizes conforme sua infra.\n",
    "- Requer pacotes listados no README associado.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# 0. Imports gerais e detecção de GPU\n",
    "# =========================\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import unidecode\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arff\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Hugging Face / PEFT / SHAP\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import shap\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.max_open_warning\": 0})\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(f\"USE_CUDA={USE_CUDA}, device={device}\")\n",
    "\n",
    "# =========================\n",
    "# 1. Carregamento OFFCOMBR\n",
    "# =========================\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/rogersdepelle/OffComBR/master\"\n",
    "ARFF_URLS = {\n",
    "    \"offcombr-2\": f\"{GITHUB_RAW_BASE}/OffComBR2.arff\",\n",
    "    \"offcombr-3\": f\"{GITHUB_RAW_BASE}/OffComBR3.arff\",\n",
    "}\n",
    "\n",
    "def load_offcombr(cfg=\"offcombr-2\"):\n",
    "    if cfg not in ARFF_URLS:\n",
    "        raise ValueError(\"Use 'offcombr-2' ou 'offcombr-3'\")\n",
    "    raw = urllib.request.urlopen(ARFF_URLS[cfg]).read().decode(\"utf-8\", errors=\"ignore\")\n",
    "    obj = arff.loads(raw)\n",
    "    cols = [a[0] for a in obj[\"attributes\"]]\n",
    "    df = pd.DataFrame(obj[\"data\"], columns=cols)\n",
    "    if \"document\" in df.columns:\n",
    "        df = df.rename(columns={\"document\": \"text\"})\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    df = df[df[\"text\"] != \"\"].copy()\n",
    "    class_col = next((c for c in df.columns if \"class\" in c.lower()), None)\n",
    "    if class_col is None:\n",
    "        raise ValueError(\"Não foi encontrada coluna de classe no ARFF\")\n",
    "    df[\"label_int\"] = df[class_col].astype(str).str.lower().map({\"no\":0,\"yes\":1,\"not\":0,\"offensive\":1}).astype(int)\n",
    "    return df[[\"text\",\"label_int\"]]\n",
    "\n",
    "# =========================\n",
    "# 2. Pré-processamento (Português)\n",
    "# =========================\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "spell = SpellChecker(language=\"pt\")\n",
    "giria_dict = {\"vc\":\"você\",\"pq\":\"porque\",\"tbm\":\"também\",\"q\":\"que\"}\n",
    "\n",
    "def preprocess_text_pt(text, do_spell=True):\n",
    "    text = str(text).lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub(r\"http\\S+|@\\w+|#\\w+|\\s+\", \" \", text)\n",
    "    for g, norm in giria_dict.items():\n",
    "        text = re.sub(rf\"\\b{g}\\b\", norm, text)\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemma = token.lemma_\n",
    "            if do_spell:\n",
    "                corr = spell.correction(token.text)\n",
    "                tokens.append(corr if corr is not None and corr != token.text else lemma)\n",
    "            else:\n",
    "                tokens.append(lemma)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def make_splits(df, test_size=0.2, seed=42):\n",
    "    X = df[\"text\"].apply(preprocess_text_pt)\n",
    "    y = df[\"label_int\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X.values, y.values, test_size=test_size, stratify=y.values, random_state=seed\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# =========================\n",
    "# 3. Modelos: Random Forest pipeline\n",
    "# =========================\n",
    "class LinguisticFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        feats = []\n",
    "        for text in X:\n",
    "            toks = text.split()\n",
    "            insults = sum(1 for w in toks if w in {\"idiota\",\"burro\",\"lixo\",\"estúpido\",\"imbecil\"})\n",
    "            upper_chars = sum(1 for c in text if c.isupper())\n",
    "            excl = text.count(\"!\")\n",
    "            feats.append([insults, upper_chars, excl])\n",
    "        return np.array(feats)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=10000)\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"features\", FeatureUnion([(\"tfidf\", tfidf), (\"ling\", LinguisticFeatures())])),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# =========================\n",
    "# 4. Função genérica de avaliação\n",
    "# =========================\n",
    "def avaliar_modelo(nome, modelo, X_test_raw, y_test, tipo=\"sklearn\"):\n",
    "    if tipo == \"sklearn\":\n",
    "        y_pred = modelo.predict(X_test_raw)\n",
    "        y_score = modelo.predict_proba(X_test_raw)[:,1]\n",
    "    elif tipo == \"trainer\":\n",
    "        preds = modelo.predict(X_test_raw)\n",
    "        logits = preds.predictions\n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[:,0,:]\n",
    "        y_score = logits[:,1]\n",
    "        y_pred = logits.argmax(axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Tipo inválido\")\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"Modelo\": nome, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-score\": f1, \"y_score\": y_score}\n",
    "\n",
    "# =========================\n",
    "# 5. Função LLaMA3 + LoRA (integração GPU-aware)\n",
    "# =========================\n",
    "def treinar_llama_lora(cfg, X_train, y_train, X_eval, y_eval, output_dir_base=\"./results_llama\"):\n",
    "    model_name = \"meta-llama/Llama-3-8b-instruct\"  # substituir conforme checkpoint disponível\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    max_len = 256\n",
    "\n",
    "    ds_train = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train})\n",
    "    ds_eval  = Dataset.from_dict({\"text\": X_eval.tolist(), \"label\": y_eval})\n",
    "    def tokenize_fn(batch): return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=max_len)\n",
    "    ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "    ds_eval  = ds_eval.map(tokenize_fn, batched=True)\n",
    "    ds_train.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    ds_eval.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    outdir = os.path.join(output_dir_base, cfg, \"llama_lora\")\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=outdir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=USE_CUDA,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_eval,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(ds_eval)\n",
    "    logits = preds.predictions\n",
    "    if logits.ndim == 3:\n",
    "        logits = logits[:,0,:]\n",
    "    y_score = logits[:,1]\n",
    "    y_pred = logits.argmax(axis=-1)\n",
    "\n",
    "    acc = accuracy_score(y_eval, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_eval, y_pred, average=\"weighted\", zero_division=0)\n",
    "    return {\"Modelo\": \"LLaMA3 + LoRA\", \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-score\": f1, \"y_score\": y_score}\n",
    "\n",
    "# =========================\n",
    "# 6. Loop principal por dataset: treino e avaliação\n",
    "# =========================\n",
    "comparacao_datasets = {}\n",
    "comparacao_datasets_raw = {}\n",
    "\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    print(f\"\\n--- Pipeline para {cfg} ---\")\n",
    "    df = load_offcombr(cfg)\n",
    "    comparacao_datasets_raw[cfg] = df.copy()\n",
    "    X_train, X_test, y_train, y_test = make_splits(df)\n",
    "\n",
    "    # Random Forest\n",
    "    print(\"Treinando Random Forest...\")\n",
    "    pipeline_rf.fit(X_train, y_train)\n",
    "    res_rf = avaliar_modelo(\"Random Forest (TF-IDF+n-grams)\", pipeline_rf, X_test, y_test, tipo=\"sklearn\")\n",
    "\n",
    "    # BERTimbau\n",
    "    print(\"Treinando BERTimbau (uso de GPU se disponível)...\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "    ds_train = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train})\n",
    "    ds_test  = Dataset.from_dict({\"text\": X_test.tolist(), \"label\": y_test})\n",
    "    def tokenize_fn(batch): return tokenizer_bert(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    ds_train = ds_train.map(tokenize_fn, batched=True)\n",
    "    ds_test  = ds_test.map(tokenize_fn, batched=True)\n",
    "    ds_train.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    ds_test.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    model_bert = BertForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)\n",
    "    model_bert.to(device)\n",
    "    bert_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{cfg}/bert\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=USE_CUDA,\n",
    "    )\n",
    "    trainer_bert = Trainer(model=model_bert, args=bert_args, train_dataset=ds_train, eval_dataset=ds_test, tokenizer=tokenizer_bert)\n",
    "    trainer_bert.train()\n",
    "    res_bert = avaliar_modelo(\"BERTimbau (fine-tune)\", trainer_bert, ds_test, y_test, tipo=\"trainer\")\n",
    "\n",
    "    # LLaMA3 + LoRA\n",
    "    print(\"Treinando LLaMA3 + LoRA (GPU se disponível; pode falhar sem infra apropriada)...\")\n",
    "    try:\n",
    "        res_llama = treinar_llama_lora(cfg, X_train, y_train, X_test, y_test)\n",
    "        resultados = [res_rf, res_bert, res_llama]\n",
    "    except Exception as e:\n",
    "        print(\"LLaMA3+LoRA falhou ou indisponível:\", e)\n",
    "        resultados = [res_rf, res_bert]\n",
    "\n",
    "    comparacao_datasets[cfg] = pd.DataFrame([{k:v for k,v in r.items() if k!=\"y_score\"} for r in resultados])\n",
    "    comparacao_datasets[cfg + \"_raw_results\"] = resultados\n",
    "    comparacao_datasets[cfg + \"_ytest\"] = y_test\n",
    "    comparacao_datasets[cfg + \"_Xtest\"] = X_test\n",
    "\n",
    "# =========================\n",
    "# 7. Seção de avaliação por dataset (métricas, ROC, PR, conf matrix)\n",
    "# =========================\n",
    "def avaliar_resultados(resultados, y_test, cfg_name):\n",
    "    df_resultados = pd.DataFrame([{k:v for k,v in r.items() if k!=\"y_score\"} for r in resultados])\n",
    "    print(f\"\\n=== Comparação de Modelos - {cfg_name.upper()} ===\")\n",
    "    print(df_resultados)\n",
    "\n",
    "    melted = df_resultados.melt(id_vars=\"Modelo\", var_name=\"Métrica\", value_name=\"Valor\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(data=melted[melted[\"Métrica\"]!=\"Modelo\"], x=\"Métrica\", y=\"Valor\", hue=\"Modelo\")\n",
    "    plt.title(f\"Comparação de Modelos - {cfg_name.upper()}\"); plt.ylim(0,1); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for r in resultados:\n",
    "        fpr, tpr, _ = roc_curve(y_test, r[\"y_score\"])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"{r['Modelo']} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0,1],[0,1], color=\"gray\", linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Curvas ROC - {cfg_name.upper()}\"); plt.legend(loc=\"lower right\"); plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for r in resultados:\n",
    "        precision, recall, _ = precision_recall_curve(y_test, r[\"y_score\"])\n",
    "        ap = average_precision_score(y_test, r[\"y_score\"])\n",
    "        plt.plot(recall, precision, lw=2, label=f\"{r['Modelo']} (AP = {ap:.2f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Curvas Precision-Recall - {cfg_name.upper()}\"); plt.legend(loc=\"lower left\"); plt.show()\n",
    "\n",
    "    for r in resultados:\n",
    "        y_pred = (r[\"y_score\"] >= 0.5).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=[\"Não-ofensivo\",\"Ofensivo\"],\n",
    "                    yticklabels=[\"Não-ofensivo\",\"Ofensivo\"])\n",
    "        plt.title(f\"Matriz de Confusão - {r['Modelo']} ({cfg_name.upper()})\")\n",
    "        plt.ylabel(\"Verdadeiro\"); plt.xlabel(\"Predito\"); plt.show()\n",
    "\n",
    "    return df_resultados\n",
    "\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    resultados = comparacao_datasets[cfg + \"_raw_results\"]\n",
    "    y_test = comparacao_datasets[cfg + \"_ytest\"]\n",
    "    _ = avaliar_resultados(resultados, y_test, cfg)\n",
    "\n",
    "# =========================\n",
    "# 8. Comparação entre datasets (tabela consolidada, distribuição, gráficos, heatmap)\n",
    "# =========================\n",
    "tabela_final = pd.concat(\n",
    "    {k: v.set_index(\"Modelo\") for k, v in comparacao_datasets.items() if not k.endswith(\"_raw_results\") and not k.endswith(\"_ytest\") and not k.endswith(\"_Xtest\")},\n",
    "    axis=0\n",
    ")\n",
    "print(\"\\n=== Tabela consolidada (OFFCOMBR-2 vs OFFCOMBR-3) ===\")\n",
    "print(tabela_final)\n",
    "\n",
    "dist_stats = []\n",
    "for cfg in [\"offcombr-2\", \"offcombr-3\"]:\n",
    "    df_raw = comparacao_datasets_raw[cfg]\n",
    "    total = len(df_raw)\n",
    "    ofensivos = int(df_raw[\"label_int\"].sum())\n",
    "    nao_ofensivos = total - ofensivos\n",
    "    pct_of = ofensivos / total\n",
    "    ratio = ofensivos / (nao_ofensivos if nao_ofensivos > 0 else 1)\n",
    "    dist_stats.append({\"Dataset\": cfg, \"Total\": total, \"Ofensivo\": ofensivos, \"Não-ofensivo\": nao_ofensivos,\n",
    "                       \"Pct_ofensivo\": pct_of, \"Pct_nao_ofensivo\": 1-pct_of, \"Ratio_of/nao\": ratio})\n",
    "    print(f\"{cfg.upper()}: total={total} | ofensivo={ofensivos} ({pct_of:.1%}) | não-ofensivo={nao_ofensivos} ({1-pct_of:.1%}) | ratio={ratio:.2f}\")\n",
    "\n",
    "df_dist = pd.DataFrame(dist_stats).set_index(\"Dataset\")\n",
    "\n",
    "plot_df = tabela_final.reset_index().rename(columns={\"level_0\":\"Dataset\"}).reset_index(drop=True)\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=plot_df, x=\"Modelo\", y=\"F1-score\", hue=\"Dataset\"); plt.title(\"Comparação de F1-score entre OFFCOMBR-2 e OFFCOMBR-3\"); plt.ylim(0,1); plt.show()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=plot_df, x=\"Modelo\", y=\"Accuracy\", hue=\"Dataset\"); plt.title(\"Comparação de Accuracy entre OFFCOMBR-2 e OFFCOMBR-3\"); plt.ylim(0,1); plt.show()\n",
    "\n",
    "num_cols = tabela_final.select_dtypes(include=[np.number]).columns\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(tabela_final[num_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\"); plt.title(\"Correlação entre métricas (OFFCOMBR-2 + OFFCOMBR-3)\"); plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "df_dist_plot = df_dist.reset_index()\n",
    "sns.barplot(data=df_dist_plot.melt(id_vars=\"Dataset\", value_vars=[\"Pct_ofensivo\",\"Pct_nao_ofensivo\"]),\n",
    "            x=\"Dataset\", y=\"value\", hue=\"variable\"); plt.ylim(0,1); plt.ylabel(\"Proporção\"); plt.title(\"Proporção de classes por dataset\"); plt.show()\n",
    "\n",
    "print(\"\\n--- Tabela final (para leitura) ---\")\n",
    "print(tabela_final.reset_index().rename(columns={\"level_0\":\"Dataset\"}))\n",
    "print(\"\\n--- Distribuição de classes ---\")\n",
    "print(df_dist)\n",
    "\n",
    "# =========================\n",
    "# 9. Explicabilidade com SHAP (gera CSVs e PNGs em ./outputs/shap/)\n",
    "# =========================\n",
    "os.makedirs(\"./outputs/shap\", exist_ok=True)\n",
    "n_background = 50\n",
    "n_explain = 100\n",
    "\n",
    "def sample_test_texts(X_test, n):\n",
    "    n = min(n, len(X_test))\n",
    "    idx = np.random.default_rng(42).choice(len(X_test), size=n, replace=False)\n",
    "    return X_test[idx], idx\n",
    "\n",
    "# 9.A Random Forest (TreeExplainer)\n",
    "try:\n",
    "    print(\"\\n=== SHAP: Random Forest ===\")\n",
    "    any_cfg = \"offcombr-2\"\n",
    "    if any_cfg + \"_Xtest\" in comparacao_datasets:\n",
    "        test_texts = comparacao_datasets[any_cfg + \"_Xtest\"]\n",
    "    else:\n",
    "        test_texts = X_test\n",
    "    X_test_full_feats = pipeline_rf.named_steps[\"features\"].transform(test_texts)\n",
    "    explainer_rf = shap.TreeExplainer(pipeline_rf.named_steps[\"clf\"])\n",
    "    shap_values_rf = explainer_rf.shap_values(X_test_full_feats, check_additivity=False)\n",
    "    sv_rf_pos = shap_values_rf[1] if isinstance(shap_values_rf, list) else shap_values_rf\n",
    "    mean_abs = np.abs(sv_rf_pos).mean(axis=0)\n",
    "    tfidf_names = pipeline_rf.named_steps[\"features\"].transformer_list[0][1].get_feature_names_out()\n",
    "    ling_names = [\"insults_count\",\"upper_chars\",\"exclamations\"]\n",
    "    feat_names = list(tfidf_names) + ling_names\n",
    "    df_shap_rf = pd.DataFrame({\"feature\": feat_names, \"mean_abs_shap\": mean_abs}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "    df_shap_rf.to_csv(\"./outputs/shap/shap_rf_feature_importance.csv\", index=False)\n",
    "    plt.figure(figsize=(8,10))\n",
    "    shap.summary_plot(sv_rf_pos, X_test_full_feats, feature_names=feat_names, max_display=30, show=False)\n",
    "    plt.tight_layout(); plt.savefig(\"./outputs/shap/shap_rf_summary.png\", dpi=150); plt.close()\n",
    "    print(\"Saved ./outputs/shap/shap_rf_feature_importance.csv and shap_rf_summary.png\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP Random Forest falhou:\", e)\n",
    "\n",
    "# 9.B BERTimbau token-level (shap.Explainer with text masker)\n",
    "try:\n",
    "    print(\"\\n=== SHAP: BERTimbau ===\")\n",
    "    # Use model_bert and tokenizer_bert from last loop (they exist in namespace)\n",
    "    bg_cfg = \"offcombr-3\" if \"offcombr-3\" in comparacao_datasets_raw else \"offcombr-2\"\n",
    "    bg_texts = np.random.choice(comparacao_datasets_raw[bg_cfg][\"text\"].values, size=min(n_background, len(comparacao_datasets_raw[bg_cfg])), replace=False)\n",
    "    test_texts, _ = sample_test_texts(comparacao_datasets[bg_cfg + \"_Xtest\"] if bg_cfg + \"_Xtest\" in comparacao_datasets else X_test, n_explain)\n",
    "    tokenizer_local = tokenizer_bert\n",
    "    model_local = model_bert\n",
    "    def bert_predict_proba(texts):\n",
    "        enc = tokenizer_local(list(texts), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        model_local.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_local(**enc)\n",
    "            probs = torch.softmax(out.logits, dim=-1)[:,1].detach().cpu().numpy()\n",
    "        return np.vstack([1-probs, probs]).T\n",
    "    masker = shap.maskers.Text(tokenizer_local)\n",
    "    explainer_bert = shap.Explainer(lambda x: bert_predict_proba(x), masker, output_names=[\"non-off\",\"offensive\"])\n",
    "    shap_values_bert = explainer_bert(test_texts)\n",
    "    tokens_list = []\n",
    "    for i in range(len(test_texts)):\n",
    "        toks = shap_values_bert.data[i]\n",
    "        vals = shap_values_bert.values[i][:,1]\n",
    "        for t, v in zip(toks, vals):\n",
    "            tokens_list.append({\"text\": test_texts[i], \"token\": t, \"shap_abs\": abs(v), \"shap\": v})\n",
    "    df_tokens = pd.DataFrame(tokens_list)\n",
    "    df_token_summary = df_tokens.groupby(\"token\")[\"shap_abs\"].mean().sort_values(ascending=False).reset_index().rename(columns={\"shap_abs\":\"mean_abs_shap\"})\n",
    "    df_token_summary.to_csv(\"./outputs/shap/shap_bert_token_importance.csv\", index=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    shap.plots.bar(shap_values_bert[:,:,1], max_display=30, show=False)\n",
    "    plt.title(\"BERTimbau token-level SHAP (class=offensive)\")\n",
    "    plt.savefig(\"./outputs/shap/shap_bert_summary_bar.png\", dpi=150); plt.close()\n",
    "    print(\"Saved ./outputs/shap/shap_bert_token_importance.csv and shap_bert_summary_bar.png\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP BERTimbau falhou:\", e)\n",
    "\n",
    "# 9.C LLaMA3 + LoRA token-level (when available)\n",
    "try:\n",
    "    has_llama = any(any(r.get(\"Modelo\",\"\")==\"LLaMA3 + LoRA\" for r in comparacao_datasets.get(cfg + \"_raw_results\", [])) for cfg in [\"offcombr-2\",\"offcombr-3\"])\n",
    "    if has_llama:\n",
    "        print(\"\\n=== SHAP: LLaMA3 + LoRA ===\")\n",
    "        llama_cfg = None\n",
    "        for cfg in [\"offcombr-2\",\"offcombr-3\"]:\n",
    "            raw = comparacao_datasets.get(cfg + \"_raw_results\", [])\n",
    "            if any(r.get(\"Modelo\",\"\")== \"LLaMA3 + LoRA\" for r in raw):\n",
    "                llama_cfg = cfg\n",
    "                break\n",
    "        model_name = \"meta-llama/Llama-3-8b-instruct\"\n",
    "        tokenizer_llama = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        llama_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, load_in_8bit=True, device_map=\"auto\")\n",
    "        def llama_predict_proba(texts):\n",
    "            enc = tokenizer_llama(list(texts), padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                out = llama_model(**enc)\n",
    "                probs = torch.softmax(out.logits, dim=-1)[:,1].detach().cpu().numpy()\n",
    "            return np.vstack([1-probs, probs]).T\n",
    "        test_texts, _ = sample_test_texts(comparacao_datasets[llama_cfg + \"_Xtest\"], n_explain)\n",
    "        masker = shap.maskers.Text(tokenizer_llama)\n",
    "        explainer_llama = shap.Explainer(lambda x: llama_predict_proba(x), masker, output_names=[\"non-off\",\"offensive\"])\n",
    "        shap_values_llama = explainer_llama(test_texts)\n",
    "        tokens_list = []\n",
    "        for i in range(len(test_texts)):\n",
    "            toks = shap_values_llama.data[i]\n",
    "            vals = shap_values_llama.values[i][:,1]\n",
    "            for t, v in zip(toks, vals):\n",
    "                tokens_list.append({\"text\": test_texts[i], \"token\": t, \"shap_abs\": abs(v), \"shap\": v})\n",
    "        df_tokens_llama = pd.DataFrame(tokens_list)\n",
    "        df_token_summary_llama = df_tokens_llama.groupby(\"token\")[\"shap_abs\"].mean().sort_values(ascending=False).reset_index().rename(columns={\"shap_abs\":\"mean_abs_shap\"})\n",
    "        df_token_summary_llama.to_csv(\"./outputs/shap/shap_llama_token_importance.csv\", index=False)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        shap.plots.bar(shap_values_llama[:,:,1], max_display=30, show=False)\n",
    "        plt.title(\"LLaMA3+LoRA token-level SHAP (class=offensive)\")\n",
    "        plt.savefig(\"./outputs/shap/shap_llama_summary_bar.png\", dpi=150); plt.close()\n",
    "        print(\"Saved ./outputs/shap/shap_llama_token_importance.csv and shap_llama_summary_bar.png\")\n",
    "    else:\n",
    "        print(\"Nenhum resultado LLaMA3+LoRA detectado; pulando SHAP para LLaMA.\")\n",
    "except Exception as e:\n",
    "    print(\"SHAP LLaMA3+LoRA falhou:\", e)\n",
    "\n",
    "# 9.D Consolidação rápida\n",
    "try:\n",
    "    print(\"\\n=== Consolidação de tabelas SHAP (top itens) ===\")\n",
    "    if os.path.exists(\"./outputs/shap/shap_rf_feature_importance.csv\"):\n",
    "        print(pd.read_csv(\"./outputs/shap/shap_rf_feature_importance.csv\").head(20))\n",
    "    if os.path.exists(\"./outputs/shap/shap_bert_token_importance.csv\"):\n",
    "        print(pd.read_csv(\"./outputs/shap/shap_bert_token_importance.csv\").head(30))\n",
    "    if os.path.exists(\"./outputs/shap/shap_llama_token_importance.csv\"):\n",
    "        print(pd.read_csv(\"./outputs/shap/shap_llama_token_importance.csv\").head(30))\n",
    "except Exception as e:\n",
    "    print(\"Consolidação SHAP falhou:\", e)\n",
    "\n",
    "print(\"\\nPipeline completo. Saídas SHAP em ./outputs/shap/ e resultados em ./results_* .\")"
   ],
   "id": "e21e9ebb15a0baa2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chita\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_CUDA=False, device=cpu\n",
      "\n",
      "--- Pipeline para offcombr-2 ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 240\u001B[39m\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m cfg \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33moffcombr-2\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33moffcombr-3\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    239\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m--- Pipeline para \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcfg\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m ---\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m     df = \u001B[43mload_offcombr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcfg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    241\u001B[39m     comparacao_datasets_raw[cfg] = df.copy()\n\u001B[32m    242\u001B[39m     X_train, X_test, y_train, y_test = make_splits(df)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 74\u001B[39m, in \u001B[36mload_offcombr\u001B[39m\u001B[34m(cfg)\u001B[39m\n\u001B[32m     72\u001B[39m raw = urllib.request.urlopen(ARFF_URLS[cfg]).read().decode(\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m, errors=\u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     73\u001B[39m obj = arff.loads(raw)\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m cols = [a[\u001B[32m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mattributes\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m]\n\u001B[32m     75\u001B[39m df = pd.DataFrame(obj[\u001B[33m\"\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m\"\u001B[39m], columns=cols)\n\u001B[32m     76\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdocument\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m df.columns:\n",
      "\u001B[31mTypeError\u001B[39m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
